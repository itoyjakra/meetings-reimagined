{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"mixtral-8x7b-32768\"\n",
    "model_name = \"llama3-70b-8192\"\n",
    "chat = ChatGroq(temperature=0, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex is an AI-powered search engine and knowledge retrieval system that uses a unique approach to indexing and retrieving information. It's designed to provide more accurate and relevant results compared to traditional search engines.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. **Indexing**: LlamaIndex creates a massive index of text data from various sources, including books, articles, research papers, and websites. This index is constantly updated to ensure freshness and relevance.\n",
      "2. **Question-Answering**: When you ask LlamaIndex a question, it uses its index to generate a response. It doesn't just provide a list of links like traditional search engines; instead, it tries to directly answer your question based on the information it has indexed.\n",
      "3. **Knowledge Retrieval**: LlamaIndex uses advanced natural language processing (NLP) and machine learning algorithms to understand the context and intent behind your question. It then retrieves the most relevant information from its index to provide a concise and accurate answer.\n",
      "\n",
      "The benefits of LlamaIndex include:\n",
      "\n",
      "* More accurate results: By directly answering questions, LlamaIndex reduces the noise and irrelevant results often found in traditional search engines.\n",
      "* Time-saving: You get the answer you need quickly, without having to sift through multiple pages of search results.\n",
      "* In-depth knowledge: LlamaIndex's vast index and advanced algorithms enable it to provide more comprehensive and detailed answers.\n",
      "\n",
      "Overall, LlamaIndex is an innovative tool that's changing the way we interact with information and find answers online.\n"
     ]
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "question = \"What are the special features of Llama 3 model?\"\n",
    "question = \"What is langsmith?\"\n",
    "question = \"What is llamaindex?\"\n",
    "reply = chain.invoke({\"text\": question})\n",
    "print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-variants",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
